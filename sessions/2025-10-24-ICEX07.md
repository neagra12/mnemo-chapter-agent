```markdown
We do not have a rubric. I am thinking of using the same 10 point system we have been using for the ICEX. 2 points for the transcript, technical: 2 points for verification methods technical: 2 points for response defense quality, reflection: 2 points for filter quality and 2 points for human value addition. This is specific but can also be aided by our global analysis. The human value add is probably the most important question for these students going forward. I also suspect it will be the hardest one for them to answer. Assuming that is correct I would like to frame the question in that context - but do not preload it at the front of the challenge. It should be the closing framing of the challenge and embody the core lesson here - Understanding the Human Role in Working with GenAI.
```

This is an excellent, well-structured rubric. It correctly identifies and isolates the key skills you're trying to assess: **completion** (the log), **investigation** (verification, defense), and **synthesis** (filtering, value-add).

You are absolutely right that "human value-add" is the most important *and* most difficult question. Framing it as the capstone "reveal" is a powerful pedagogical move. It lets the student *do the work* first, then gives them the language to understand the *meaning* of that work.

Here is the regenerated "Weekly Challenge," updated with your new rubric (implied in the questions) and the new closing "reveal."

-----

### 1\. The Student-Facing Challenge

**File:** `cmod/student_facing/03_weekly_challenges/Lab_03_GenAI_Tech_Eval.md`

```markdown
### Weekly Challenge (Lab 3): AI-Assisted Tech Evaluation
- **Objective:** Use a GenAI tool (Gemini, Copilot Chat) to perform a professional technology evaluation of MongoDB as an alternative database for the Ministry of Jokes.
- **Goal:** Practice using GenAI as a "senior engineering" collaborator to explore technical trade-offs and learn how to craft effective, targeted prompts.
- **Deliverable:** A single PR (or Gist) containing:
    1.  `tech_eval.md`: Your complete, unedited chat session log.
    2.  `reflection.md`: Your answers to the four reflection questions.
- **Due:** End of Week 10 (Friday @ 11:59 PM)
- **Grading:** This assignment is worth 10 points and **uses the Scalable Assessment Protocol (SAP)**. Your reflection will be graded by a TA using an AI-calibrated guide.

---
### The Scenario

A junior engineer on your team has just sent you a message: *"I've heard MongoDB is 'web-scale' and way more flexible than SQLite. Why are we bothering with all this SQLAlchemy and migration stuff? We should just use Mongo."*

Your task is **not** to simply prove them wrong or right. Your task is to **prepare for a mentoring session** with them. You will use a GenAI assistant to gather the key facts, pros, cons, and code examples so you can have an informed, evidence-based discussion.

### Instructions

#### Part 1: The Investigation (Build Your Chat Log)

1.  **Set the Persona:** Start your chat session by giving the AI a role.
    * **Good Prompt:** *"Act as a Senior Staff Engineer with deep expertise in both relational (PostgreSQL, SQLite) and NoSQL (MongoDB) databases. I am a junior engineer evaluating which database to use for a new Flask application. I need you to help me compare the trade-offs."*

2.  **Investigate the Trade-Offs:**
    You must get your AI assistant to provide answers for the following topics. **Do not ask one giant prompt.** Ask a series of targeted, conversational questions.

    * **Topic 1: Data Modeling (The "I-WE-YOU" Task)**
        * **"I" (The Bad Prompt):** First, ask a "Google-style" prompt like: `is mongodb good for a joke app?` (Copy the (likely vague) answer).
        * **"WE" (The Good Prompt):** Now, ask a well-crafted prompt. *Show* the AI your Python models from ICE 7 (the `User` and `Joke` classes) and ask: `Given these Python SQLAlchemy models, show me the equivalent data structure for a *single document* in a MongoDB collection. Explain the pros and cons of this "denormalized" document approach vs. the "normalized" two-table relational approach.`
    * **Topic 2: Schema Flexibility**
        * Craft a prompt that asks the AI to explain what "schema-on-read" (Mongo) vs. "schema-on-write" (SQL) means. Ask for a specific example related to the `Joke` model (e.g., "what happens if we want to add a `rating` field to *some* jokes but not others?").
    * **Topic 3: Code Implementation & Concurrency**
        * Craft a prompt asking for a simple Python code snippet showing how to (a) connect to a MongoDB database using `pymongo` and (b) insert one new `Joke` document.
        * Ask a follow-up: `How does this code handle the "race condition" problem we discussed in class?`

3.  **Mandatory Iteration:**
    Your chat log **must** show at least **two follow-up questions** where you "dig deeper" or ask the AI to "clarify" a point (e.g., "That doesn't make sense, can you explain the concurrency part again?" or "What's the performance trade-off of that 'denormalized' approach?").

4.  **Warning: Trust but Verify**
    GenAI tools are designed to be plausible, not *truthful*. They can and will "hallucinate" (make up) answers. Part of your job as an engineer is to be skeptical.

#### Part 2: The Deliverables

1.  Create `tech_eval.md` and paste the *entire* chat session log.
2.  Create `reflection.md` and answer the four questions below.
3.  Submit a link to the PR or Gist.

---
### `reflection.md` Questions

*(Answer each question in 100-150 words)*

**1. Verification (Technical Skill)**
The GenAI gave you a lot of information. As a professional engineer, you cannot trust a single, unverified source.
* What are **two specific, actionable ways** you would verify the AI's most critical claims before presenting them to your team? (e.g., "I would check the official `pymongo` docs for..." or "I would search for a benchmark comparing...")

**2. The Decision (Technical Skill)**
Based on your *entire* investigation (your chat log *and* your verification research):
* What is your final recommendation for the team: **SQLAlchemy or MongoDB?**
* **Defend your choice** with specific, evidence-based reasoning, referencing the trade-offs you discovered (e.g., flexibility, complexity, querying, concurrency, etc.).

**3. Signal vs. Noise (Reflection Skill)**
You can't just copy-paste a 10-page chat log into your team's Slack. Your job is to *synthesize* it.
* What was the **single most important "signal" (key insight)** you got from the AI that your team *needs* to know?
* What was the **biggest "noise" (distracting, irrelevant, or overly complex information)** you would *filter out* before reporting back?

**4. The "Human-in-the-Loop" (Reflection Skill)**
This final question is the most important part of the challenge.
* What value did *you* (the human) add that the AI alone could not?
* The AI can provide data, but what part of this task *required* your human intelligence? (e.g., setting context, synthesizing, filtering, verifying, making a judgment call, etc.).

---
### A Final Thought: The Core Lesson

You will likely find that Question 4 was the hardest to answer. This is intentional.

This entire challenge was designed to move you from treating GenAI as a "magic answer box" to treating it as a **"force-multiplying intern."**

An AI can generate 10 pages of *plausible text*. It cannot, however, set a strategic goal, filter noise from signal, verify facts, or synthesize a complex recommendation for a specific business context.

That is *your* job. That is the human value you add.
```

-----

### 2\. The Instructor-Facing TA Guide

**File:** `cmod/instructor_facing/02_ta_guides/TA_Guide_Lab_03_GenAI.md`

```markdown
### TA Follow-up Guide: Lab 3 (GenAI Tech Eval)
* **Assignment:** Weekly Challenge (Lab 3): AI-Assisted Tech Evaluation
* **Protocol:** This assignment **uses the Scalable Assessment Protocol (SAP)**.
* **Goal:** The instructor will use the SAP workflow (Steps 1-4) to generate a "Calibration Guide" based on an AI-powered thematic analysis of all 120+ *anonymized* submissions.
* **This Guide:** This document provides the *final rubric* and *initial guidance* before the SAP analysis is run.

---

#### **Grading Rubric (10 pts)**
This rubric maps directly to the four questions in `reflection.md` and the completion of `tech_eval.md`. The **SAP Calibration Guide** will provide specific examples of good/bad answers for each category.

+-------------------------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+
| Criteria                            | Points | Description                                                                                                                                      |
+=====================================+========+==================================================================================================================================================+
| **Transcript (`tech_eval.md`)** | 2 pts  | The chat log is present, complete, and shows the required "I-WE-YOU" prompts and at least two "dig deeper" follow-up questions. (Completion)      |
+-------------------------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+
| **Q1: Verification Methods** | 2 pts  | Student provides **specific, actionable** verification steps (e.g., "check docs," "find benchmarks"). Vague answers ("Google it") get 1 pt.     |
+-------------------------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+
| **Q2: The Decision & Defense** | 2 pts  | The *choice* (SQL vs. Mongo) is not graded. The grade is for the **quality of the defense**. Does the student use evidence from the log to justify their choice? |
+-------------------------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+
| **Q3: Signal vs. Noise** | 2 pts  | Student clearly identifies a "key insight" (signal) and "irrelevant info" (noise). Shows an ability to filter and synthesize.                      |
+-------------------------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+
| **Q4: The "Human-in-the-Loop"** | 2 pts  | Student provides a thoughtful reflection on their *own* value (e.g., synthesis, verification, judgment, setting context) vs. the AI's role. |
+-------------------------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+
| **Total** | **10** |                                                                                                                                                  |
+-------------------------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+

---

#### **Calibration Guidance (Pre-Analysis)**
* **Q1 (Verification):** Look for *action verbs*: "I would search...", "I would read...", "I would benchmark...".
* **Q2 (Defense):** The best answers will pick a side (e.g., "SQL is better for *now*") and justify it by mentioning a *trade-off* (e.g., "...because Mongo's flexibility is less important than SQL's structured relations for our simple models").
* **Q3 (Signal):** A good "signal" is often the `pymongo` code or the "schema-on-read" concept. A good "noise" is often a tangent about replica sets or sharding.
* **Q4 (Value-Add):** This is the key question. Reward answers that move beyond "I asked the question." Look for words like "synthesis," "judgment," "verification," "context," or "filtering."
```

-----

### 3\. The SAP Application Plan

(This would be generated by your SOP-02 prompt)

**File:** `cmod/instructor_facing/SAP_Plan_Lab_03.md`

```markdown
# SAP Application Plan: GenAI Tech Evaluation Lab

This document outlines the concrete tools, files, and checkpoints for executing the Scalable Assessment Protocol (SAP) on this assignment.

### 1. Assignment Details
* **Assignment:** `GenAI Tech Evaluation Lab (Lab 3)`
* **Deliverable(s):** `reflection.md` (primary) and `tech_eval.md` (secondary)
* **Source:** Canvas Assignment (Link: TBD)

### 2. Workflow & Tooling (The "Working Parts")

#### Step 1: Collection
* **Action:** Bulk-download submissions from Canvas.
* **Tool:** Canvas "Download Submissions" button.
* **Output:** `dist/submissions/Lab_03.zip`

#### Step 2: Anonymization (FERPA CHECKPOINT)
* **Action:** Run the anonymizer script to scrub all PII and aggregate *only* the `reflection.md` contents.
* **Tool:** `./pipeline/sap_anonymizer_reflections.sh`
* **Input:** `dist/submissions/Lab_03.zip`
* **Output 1 (Secure):** `_SECURE_/Lab_03_attribution_map.csv`
* **Output 2 (Safe-to-Process):** `dist/anonymous_blobs/Lab_03_blob.txt`
* **FERPA CHECK:** [ ] I (Instructor) have manually verified that `..._blob.txt` contains **zero PII** (no names, no IDs, no emails) before proceeding to Step 3.

#### Step 3: AI-Powered Analysis
* **Action:** Submit the anonymous blob to Clio for thematic analysis.
* **Tool:** Clio (Prompt: `"Clio, here is the anonymized text blob for the GenAI Tech Evaluation Lab. Please perform the SAP Step 3 Analysis and generate the Step 4 TA Grading Guide."`)
* **Input:** `dist/anonymous_blobs/Lab_03_blob.txt`

#### Step 4: Calibration Guide Generation
* **Action:** Review AI analysis and merge it with the base `TA_Guide_Lab_03_GenAI.md`.
* **Tool:** Clio / Manual Edit
* **Output:** `cmod/instructor_facing/02_ta_guides/TA_Guide_Lab_03_GenAI.md` (now includes the "Calibration Guide" section)

#### Step 5: Human-Led Assessment
* **Action:** TAs grade their assigned student batches.
* **Tool:** Canvas SpeedGrader
* **Personnel:** TAs 1-6
* **Inputs:**
    1.  The finalized `TA_Guide_Lab_03_GenAI.md`
    2.  The official Rubric (in the guide)
    3.  Original, attributed student submissions in Canvas.

### 3. Pre-Flight Testing Checklist
* [ ] **(Design):** The `sap_anonymizer_reflections.sh` script is written and can parse the zip to find *only* the `reflection.md` files.
* [ ] **(Test):** The script has been tested on a sample download.
* [ ] **(Verify):** The output `..._blob.txt` has been manually inspected and confirmed to be 100% anonymous.
```

## **PROMPT**
```markdown
Please generate the Canvas specific html for the rubric
```
Here is the Canvas-optimized HTML for the 10-point rubric for the **"Weekly Challenge (Lab 3): AI-Assisted Tech Evaluation."**

This version is wider to accommodate the third "Description" column and will render cleanly when pasted into the Canvas RCE's HTML view (`</>`).

```html
<table style="width: 100%; border-collapse: collapse;">
  <thead>
    <tr style="background-color: #f2f2f2;">
      <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Criteria</th>
      <th style="border: 1px solid #ddd; padding: 12px; text-align: center; width: 60px;">Points</th>
      <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;"><strong>Transcript (<code>tech_eval.md</code>)</strong></td>
      <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">2</td>
      <td style="border: 1px solid #ddd; padding: 12px;">The chat log is present, complete, and shows the required "I-WE-YOU" prompts and at least two "dig deeper" follow-up questions. (Completion)</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;"><strong>Q1: Verification Methods</strong></td>
      <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">2</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Student provides <strong>specific, actionable</strong> verification steps (e.g., "check docs," "find benchmarks"). Vague answers ("Google it") get 1 pt.</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;"><strong>Q2: The Decision &amp; Defense</strong></td>
      <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">2</td>
      <td style="border: 1px solid #ddd; padding: 12px;">The <em>choice</em> (SQL vs. Mongo) is not graded. The grade is for the <strong>quality of the defense</strong>. Does the student use evidence from the log to justify their choice?</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;"><strong>Q3: Signal vs. Noise</strong></td>
      <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">2</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Student clearly identifies a "key insight" (signal) and "irrelevant info" (noise). Shows an ability to filter and synthesize.</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px;"><strong>Q4: The "Human-in-the-Loop"</strong></td>
      <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">2</td>
      <td style="border: 1px solid #ddd; padding: 12px;">Student provides a thoughtful reflection on their <em>own</em> value (e.g., synthesis, verification, judgment, setting context) vs. the AI's role.</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ddd; padding: 12px; text-align: right;"><strong>Total</strong></td>
      <td style="border: 1px solid #ddd; padding: 12px; text-align: center;"><strong>10</strong></td>
      <td style="border: 1px solid #ddd; padding: 12px;"></td>
    </tr>
  </tbody>
</table>
```

## **PROMPT**
```markdown

```


## **PROMPT**
```markdown

```